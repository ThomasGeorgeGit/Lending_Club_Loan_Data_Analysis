# -*- coding: utf-8 -*-
"""Lending Club Loan Data Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DNBwuXLnqQswtv0z-qzR4c_hyUs2bJEU

**Loan lending**
Create a model that predicts whether or not a loan will be default using the historical data.
For companies like Lending Club correctly predicting whether or not a loan will be a default is very important. In this project, using the historical data from 2007 to 2015, you have to build a deep learning model to predict the chance of default for future loans. As you will see later this dataset is highly imbalanced and includes a lot of features that make this problem more challenging.

Domain: Finance

Analysis to be done: Perform data preprocessing and build a deep learning prediction model. 

Content: 

Dataset columns and definition:

 

    credit.policy: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.

    purpose: The purpose of the loan (takes values "credit_card", "debt_consolidation", "educational", "major_purchase", "small_business", and "all_other").

    int.rate: The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates.

    installment: The monthly installments owed by the borrower if the loan is funded.

    log.annual.inc: The natural log of the self-reported annual income of the borrower.

    dti: The debt-to-income ratio of the borrower (amount of debt divided by annual income).

    fico: The FICO credit score of the borrower.

    days.with.cr.line: The number of days the borrower has had a credit line.

    revol.bal: The borrower's revolving balance (amount unpaid at the end of the credit card billing cycle).

    revol.util: The borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available).

    inq.last.6mths: The borrower's number of inquiries by creditors in the last 6 months.

    delinq.2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years.

    pub.rec: The borrower's number of derogatory public records (bankruptcy filings, tax liens, or judgments).
"""



"""Import Libraries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn import preprocessing
from sklearn.feature_selection import chi2
from sklearn.feature_selection import RFE
from sklearn.preprocessing import OrdinalEncoder
from sklearn.linear_model import LogisticRegression
from scipy import stats
from scipy.stats import skew, norm
import warnings

# ignore all warnings
warnings.filterwarnings("ignore")

import pandas as pd

# Mount your Google Drive to Colab notebook
from google.colab import drive
drive.mount('/content/drive')

# Read in the Excel file using pandas
df = pd.read_csv('/content/loan_data.csv',error_bad_lines=False)


# Display the loaded data in the DataFrame
print(df.head())

df.head()

df.info()

df.describe()

"""EDA
 Create a countplot using sns showing the counts of loans by purpose, with the color hue defined by not.fully.paid
"""

df['not.fully.paid'].value_counts()

sns.countplot(x="purpose", hue='not.fully.paid', data=df, palette="Set2")

df.groupby('not.fully.paid')['not.fully.paid'].count()/len(df)

sns.set_style('darkgrid')
sns.countplot(x='not.fully.paid', data=df)

""" This dataset is highly imbalanced and includes features that make this problem more challenging.
 
"""

sns.jointplot(x=df['fico'],y=df['int.rate'])

sns.lmplot(x='fico',y='int.rate',data=df,col='not.fully.paid',hue='credit.policy')

plt.figure(figsize=(10,6))
df[df['not.fully.paid']==1]['fico'].hist(alpha=0.5,color='blue',bins=30,label='not.fully.paid=1')
df[df['not.fully.paid']==0]['fico'].hist(alpha=0.5,color='red',bins=30,label='not.fully.paid=0')
plt.legend()
plt.xlabel('FICO')

""" Missing Data"""

sns.heatmap(df.isnull())

"""checking the categorical columns, purpose unique values for encoding"""

df['purpose'].unique()

"""Resample the training set

There are two approaches to make a balanced dataset out of an imbalanced one are under-sampling and over-sampling
Under-sampling

Under-sampling balances the dataset by reducing the size of the abundant class. This method is used when the quantity of data is sufficient.

Over-sampling

Oversampling is used when the quantity of data is insufficient. It tries to balance the dataset by increasing the size of rare samples.

There is no absolute advantage of one resampling method over another.
"""

count_class_0, count_class_1 = df['not.fully.paid'].value_counts()
df_0 = df[df['not.fully.paid'] == 0]
df_1 = df[df['not.fully.paid'] == 1]
df_1_over = df_1.sample(count_class_0, replace=True)
df_test_over = pd.concat([df_0, df_1_over], axis=0)
print('Random over-sampling:')
print(df_test_over['not.fully.paid'].value_counts())

sns.set_style('darkgrid')
sns.countplot(x='not.fully.paid', data=df_test_over)

df['purpose'].unique()

"""data to include numerical data only"""

num_data = df[["int.rate", "installment", "log.annual.inc", "dti", "fico", "days.with.cr.line", "revol.bal", 
               "revol.util", "not.fully.paid"]]
num_data

"""features in the numerical data"""

num_data_features = num_data.columns
num_data_features

"""separate data to include categorical data only"""

cat_data = df[["credit.policy", "purpose", "inq.last.6mths", "delinq.2yrs", "not.fully.paid"]]
cat_data

"""Outliers

subplot grid
"""

fig, axs = plt.subplots(nrows = 3, ncols = 3, figsize = (15, 12), sharex = True)
fig.subplots_adjust(hspace = 0.5)
for i, col in enumerate(num_data):
    ax = plt.subplot(3, 3, i+1)
    sns.boxplot(y = df[col])
    ax.set_title(f"Boxplot for {col}")
plt.show()

"""outliers exist in the variables such as the following: int.rate, installment, log.annual.inc, fico, days.with.cr.line and revol.bal. These outliers will be handles later."""

# Converting categorical feature into numerical feature
cat_data = cat_data.copy()
le = preprocessing.LabelEncoder()
cat_data["purpose"] = le.fit_transform(cat_data["purpose"].astype(str))
cat_data.head()

# Check the statistics of the numerical data
cat_data.describe()

"""The standard deviation in all the variables is small because the data ranges from either 0 to 1 and 0 to 6 or 0 to 33 and 0 to 13."""

# Check the distribution of the categorical data
cat_data.hist(figsize = (30, 30), bins = 20, legend = False)
plt.rcParams["font.size"] = "20"
plt.show()

"""1.categorical data is positively skewed.

2.Most clients satisfied the credit policy.

3.Most clients decided to take the loan for purposes of loan consolidation.
"""

# Create plots showing the uncertainity in the categorical data and the outliers.
plt.figure(figsize = (10, 10))
cat_data.boxplot()
plt.xticks(rotation = 90)
plt.title("Box plot showing the outliers in the categorical data")
plt.show()

# Set correlation variable
corr = df.corr()

# Plot the heatmap
plt.figure(figsize=(14,14))
sns.heatmap(corr, 
        xticklabels=corr.columns,
        yticklabels=corr.columns,
        cmap='coolwarm')

"""Categorical Features"""

cat_feats = ['purpose']

final_data = pd.get_dummies(df,columns=cat_feats,drop_first=True)

df.info()

"""
Categorical Features

We need to transform purpose columnusing dummy variables so sklearn will be able to understand them.
"""

col_fea = ['purpose']
final_data = pd.get_dummies(df_test_over,columns=col_fea,drop_first=True)
final_data.info()

final_data.corr()
plt.figure(
        figsize=[16,12]
)
sns.heatmap(data=final_data.corr(),cmap='viridis',annot=False, fmt='.2g')

"""Modeling
Deep Learning Implementation

Finally, do the train test split and fit the model with the data shape we created above. since there are 19 features, I chose the first layer of the neural network with 19 nodes.
"""

to_train = final_data[final_data['not.fully.paid'].isin([0,1])]
to_pred = final_data[final_data['not.fully.paid'] == 2]

X = to_train.drop('not.fully.paid', axis=1).values
y = to_train['not.fully.paid'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 101)

from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Activation, Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = Sequential()

model.add(
        Dense(19, activation='relu')
)

model.add(
        Dense(10, activation='relu')
)

model.add(
        Dense(5, activation='relu')
)


model.add(
        Dense(1, activation='sigmoid')
)

model.compile(
        optimizer='adam', 
        loss='binary_crossentropy', 
        metrics=['accuracy']
)

early_stop = EarlyStopping(
        monitor='val_loss', 
        mode='min', 
        verbose=1, 
        patience=25
)

model.fit(
        X_train, 
        y_train, 
        epochs=200, 
        batch_size=256, 
        validation_data=(X_test, y_test),
         callbacks=[early_stop])

pd.DataFrame(model.history.history)[['loss','val_loss']].plot()

"""validation result, the Loss plot, shows us the model is overfitted."""

predictions = np.argmax(model.predict(X_test), axis=-1)

print(
    confusion_matrix(y_test,predictions),
    '\n',
    classification_report(y_test,predictions)
)

"""overall f1-score for accuracy is 0.69

Type 2 errors (624) in the prediction

Model Refinement

Add Dropout layers to bring down the overfitting OR Lower the cut-off line in binary prediction to reduce the Type 2 error, at the cost of increasing Type 1 error.
"""

from keras.layers import Dropout

model_new = Sequential()

model_new.add(
        Dense(19, activation='relu')
)

model_new.add(Dropout(0.2))

model_new.add(
        Dense(10, activation='relu')
)

model_new.add(Dropout(0.2))

model_new.add(
        Dense(5, activation='relu')
)

model_new.add(Dropout(0.2))

model_new.add(
        Dense(1, activation='sigmoid')
)

model_new.compile(
        optimizer='adam', 
        loss='binary_crossentropy', 
        metrics=['binary_accuracy']
)


model_new.fit(
        X_train, 
        y_train, 
        epochs=200, 
        batch_size=256, 
        validation_data=(X_test, y_test),
         callbacks=[early_stop]
)

pd.DataFrame(model_new.history.history)[['loss','val_loss']].plot()

"""The graph shows that, by adding in Dropout layers, we have reduced the overfitting issue compared with the old model."""

predictions_new = (model_new.predict(X_test) >= 0.2).astype('int')

print(
    confusion_matrix(y_test,predictions_new),
    '\n',
    classification_report(y_test,predictions_new)
)

"""By changing the cut-off line to 0.2 (default is 0.5), we have dramatically brought down the Type 2 error."""

dump(scaler, open('scaler.pkl', 'wb'))
model_new.save('my_model_lending_club.h5')

from tensorflow.keras.models import load_model

later_scaler = load(open('scaler.pkl', 'rb'))
later_model = load_model('my_model_lending_club.h5')

X_OOT = to_pred.drop('not.fully.paid', axis=1).values
to_pred.drop('not.fully.paid', axis=1).values

print(X_OOT.shape)

